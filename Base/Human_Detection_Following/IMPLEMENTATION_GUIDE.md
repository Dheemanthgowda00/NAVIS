# ğŸ¤– NAVIS Robot - Human Detection & Following System
## Complete Implementation Summary

---

## âœ… What Was Built

A complete **autonomous human detection and following system** that integrates with your NAVIS robot:

### Core Components
1. **MediaPipe Holistic Pose Detection** - Real-time human pose estimation
2. **Position Analysis** - LEFT / CENTER / RIGHT detection
3. **Depth Estimation** - NEAR / MEDIUM / FAR distance calculation
4. **Intelligent Command Generation** - Automatic MQTT robot control
5. **Web Control Interface** - Live monitoring and threshold adjustment
6. **MQTT Integration** - Seamless communication with ESP32

---

## ğŸ“ Project Structure

```
/home/navis/NAVIS/Base/Human_Detection_Following/
â”œâ”€â”€ app.py                      # Main Flask app (312 lines)
â”‚   â”œâ”€â”€ MediaPipe integration
â”‚   â”œâ”€â”€ Position detection algorithm
â”‚   â”œâ”€â”€ Depth estimation logic
â”‚   â”œâ”€â”€ MQTT command generation
â”‚   â”œâ”€â”€ Flask routes
â”‚   â””â”€â”€ Video streaming
â”‚
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html             # Web interface (~450 lines)
â”‚       â”œâ”€â”€ Live video stream
â”‚       â”œâ”€â”€ Real-time status display
â”‚       â”œâ”€â”€ Threshold controls
â”‚       â”œâ”€â”€ Modern UI design
â”‚       â””â”€â”€ JavaScript auto-update
â”‚
â”œâ”€â”€ test_setup.py              # System verification script
â”œâ”€â”€ viewer.py                  # Standalone pose viewer (no web)
â”œâ”€â”€ requirements.txt           # Dependencies
â”œâ”€â”€ QUICKSTART.md             # 5-minute setup guide
â”œâ”€â”€ SETUP_COMPLETE.md         # Setup summary
â””â”€â”€ README                     # (in main README.md)
```

---

## ğŸ”„ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              HUMAN DETECTION PIPELINE                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                       â”‚
â”‚  Camera (USB/Pi Camera)                             â”‚
â”‚       â†“                                              â”‚
â”‚  OpenCV: Capture Frame (640Ã—480, 15-25 FPS)        â”‚
â”‚       â†“                                              â”‚
â”‚  MediaPipe: Pose Detection (33 landmarks)          â”‚
â”‚       â†“                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚ Analyze Pose Landmarks:              â”‚           â”‚
â”‚  â”‚ - Nose (0)         â†’ Position X      â”‚           â”‚
â”‚  â”‚ - L Shoulder (11)  â†’ Depth estimate  â”‚           â”‚
â”‚  â”‚ - R Shoulder (12)  â†’ Depth estimate  â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚       â†“                                              â”‚
â”‚  Position Detection: LEFT / CENTER / RIGHT           â”‚
â”‚       â†“                                              â”‚
â”‚  Depth Detection: NEAR / MEDIUM / FAR                â”‚
â”‚       â†“                                              â”‚
â”‚  Decision Engine (Command Logic)                     â”‚
â”‚       â†“                                              â”‚
â”‚  MQTT Publisher: Publish JSON Command               â”‚
â”‚       â†“                                              â”‚
â”‚  {"cmd": "X", "speed": 200}                         â”‚
â”‚       â†“                                              â”‚
â”‚  Mosquitto MQTT Broker (localhost:1883)             â”‚
â”‚       â†“ Publish to: robot/control                    â”‚
â”‚       â†“                                              â”‚
â”‚  ESP32 MQTT Subscriber                              â”‚
â”‚       â†“ Parse JSON & Execute                         â”‚
â”‚       â†“                                              â”‚
â”‚  BTS7960 Motor Drivers                              â”‚
â”‚       â†“                                              â”‚
â”‚  DC Motors â†’ Robot Movement                          â”‚
â”‚                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Detection Algorithm

### Position Detection (Horizontal)
```
Frame divided into 3 zones based on nose X coordinate:

    NOSE_X_NORM
    (0.0 to 1.0)
        â†“
    
0.0 â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ 1.0
    â”‚  LEFT   â”‚ CENTER  â”‚  RIGHT  â”‚
    â”‚ 0-0.35  â”‚0.35-0.65â”‚0.65-1.0 â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    Tolerance: Â±15% for center zone
```

### Depth Detection (Distance)
```
Shoulder Width Normalized â†’ Depth Percentage

SHOULDER_WIDTH_NORM
(inverted to get distance)
        â†“
        
0.0% â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ 100%
     â”‚  NEAR   â”‚ MEDIUM  â”‚  FAR    â”‚
     â”‚ < 30%   â”‚ 30-70%  â”‚ > 70%   â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     
     Closer = Wider shoulders = Smaller %
     Farther = Narrower shoulders = Larger %
```

### Command Generation Logic
```
IF person_detected == False:
    COMMAND = 'S' (STOP)

ELIF depth == 'FAR':
    COMMAND = 'F' (FORWARD)  â† Follow forward

ELIF depth == 'NEAR':
    COMMAND = 'B' (BACKWARD) â† Back away

ELIF depth == 'MEDIUM':
    IF position == 'LEFT':
        COMMAND = 'L' (LEFT)
    ELIF position == 'RIGHT':
        COMMAND = 'R' (RIGHT)
    ELSE:  # CENTER
        COMMAND = 'S' (STOP)  â† Track at this distance

PUBLISH: {"cmd": COMMAND, "speed": 200}
```

---

## ğŸ”Œ Hardware Integration

### Required Hardware
âœ… **Camera**: USB or Raspberry Pi camera module
âœ… **ESP32**: Microcontroller with WiFi/MQTT
âœ… **BTS7960**: H-bridge motor driver (2 units for 2 motors)
âœ… **Motors**: DC motors with wheels
âœ… **Raspberry Pi**: Running MQTT broker + Flask app
âœ… **Network**: WiFi connection (2.4GHz)

### Message Format
```json
{
  "cmd": "F",        // Command: F, B, L, R, or S
  "speed": 200       // Speed: 0-255
}
```

### MQTT Topic
- **Topic**: `robot/control`
- **Broker**: localhost:1883
- **Publisher**: Flask app on Raspberry Pi
- **Subscriber**: ESP32 MQTT client

---

## ğŸš€ Quick Start Commands

### 1. Install Dependencies
```bash
cd /home/navis/NAVIS
source venv_3.10/bin/activate
pip install -r Base/Human_Detection_Following/requirements.txt
```

*Note: OpenCV and MediaPipe are large - installation takes 5-10 minutes on Raspberry Pi*

### 2. Test System
```bash
cd Base/Human_Detection_Following
python test_setup.py
```

Expected output:
```
âœ“ Camera working: Resolution 640x480
âœ“ MediaPipe imported successfully
âœ“ Holistic model loaded
âœ“ Pose detection working
âœ“ OpenCV version: 4.8.1.78
âœ“ MQTT broker connected
âœ“ Test message published

âœ“ All systems ready! Start the app...
```

### 3. Run Application
```bash
python app.py
```

Expected output:
```
âœ… MQTT Connected
ğŸ¥ Human Detection Following - Starting on http://0.0.0.0:5051
 * Running on http://0.0.0.0:5051
 * Press CTRL+C to quit
```

### 4. Access Web Interface
Open browser: **http://192.168.0.199:5051**

### 5. Monitor MQTT (Optional)
```bash
mosquitto_sub -h 127.0.0.1 -p 1883 -t "robot/control"
```

---

## ğŸ¨ Web Interface Features

### Live Video Stream
- Real-time camera feed (30 FPS)
- MediaPipe skeleton overlay
- Zone visualization (LEFT, CENTER, RIGHT)
- Pose landmarks with connections

### Status Panel
- **Detection Indicator**: Red (no person) / Green (detected)
- **Position Display**: LEFT / CENTER / RIGHT
- **Depth Display**: NEAR / MEDIUM / FAR
- **Distance Percentage**: 0-100% shoulder width
- **FPS Counter**: Real-time performance
- **Command Display**: Current MQTT command

### Threshold Controls
- **Near Threshold** slider (0.1-0.5)
  - Controls when person is considered NEAR
  - Higher = backs away sooner
- **Far Threshold** slider (0.5-0.9)
  - Controls when person is considered FAR
  - Lower = follows sooner
- Real-time updates (no page reload needed)

### Design
- Dark theme (OLED-friendly)
- Green (#00ff88) and cyan (#00ccff) accent colors
- Responsive layout (desktop and mobile)
- Smooth animations and indicators

---

## âš™ï¸ Configuration Options

### In `app.py`
```python
# Frame dimensions
FRAME_WIDTH = 640                    # Camera width (pixels)
FRAME_HEIGHT = 480                   # Camera height (pixels)
FPS = 30                             # Target frame rate

# Position detection
CENTER_TOLERANCE = 0.15              # Â±15% tolerance for center

# Depth detection (adjustable via web)
DEPTH_THRESHOLD_NEAR = 0.3           # < 30% = NEAR
DEPTH_THRESHOLD_FAR = 0.7            # > 70% = FAR

# MediaPipe confidence
min_detection_confidence = 0.5        # 50% confidence threshold
min_tracking_confidence = 0.5         # 50% tracking threshold
```

### Via Web Interface
- Adjust Near Threshold: 0.1 to 0.5
- Adjust Far Threshold: 0.5 to 0.9
- Changes take effect immediately

---

## ğŸ“Š Performance Specifications

### Raspberry Pi 4B
| Metric | Expected |
|--------|----------|
| FPS | 15-25 |
| Latency | 100-200ms |
| CPU Usage | 60-80% |
| Memory Usage | 150-200 MB |
| Detection Accuracy | 85-95% |

### Optimization Tips
1. **Reduce resolution** â†’ Faster processing
2. **Increase frame skip** â†’ Lower FPS, higher throughput
3. **Use lighter model** â†’ Faster but less accurate
4. **Adjust confidence** â†’ Lower = easier detection

---

## ğŸ§ª Testing Checklist

Before deployment, verify:

- [ ] Camera displays live feed in web interface
- [ ] Person's skeleton visible with 33 landmarks
- [ ] Moving left updates position to "LEFT"
- [ ] Moving center updates position to "CENTER"
- [ ] Moving right updates position to "RIGHT"
- [ ] Moving away increases depth percentage
- [ ] Moving closer decreases depth percentage
- [ ] FAR triggers â†’ MQTT publishes 'F' command
- [ ] NEAR triggers â†’ MQTT publishes 'B' command
- [ ] Moving left at medium distance â†’ 'L' command
- [ ] Moving right at medium distance â†’ 'R' command
- [ ] Centered at medium distance â†’ 'S' command
- [ ] Robot responds to each command correctly
- [ ] Threshold sliders update detection behavior
- [ ] FPS counter shows 15+ (acceptable)

---

## ğŸ› Troubleshooting

| Problem | Cause | Solution |
|---------|-------|----------|
| Camera feed not loading | Camera not found | Check connection, run `test_setup.py` |
| No person detected | Poor lighting | Increase brightness/lighting |
| Skeleton flickering | Low FPS | Increase lighting, reduce resolution |
| Robot not responding | MQTT not working | Verify broker running, check ESP32 |
| Threshold not updating | Browser cache | Hard refresh (Ctrl+F5) |
| High CPU usage | Heavy processing | Reduce resolution, skip frames |
| Slow FPS | Heavy computation | Lower FRAME_WIDTH/HEIGHT in app.py |

---

## ğŸ“š Documentation Files

| File | Purpose |
|------|---------|
| `QUICKSTART.md` | 5-minute setup guide |
| `SETUP_COMPLETE.md` | Implementation summary |
| `HUMAN_DETECTION_GUIDE.md` | Technical deep-dive |
| `app.py` | Main application code |
| `templates/index.html` | Web interface |
| `test_setup.py` | System verification |
| `viewer.py` | Standalone debugging viewer |
| `requirements.txt` | Dependencies |

---

## ğŸ”„ Running Both Modules Simultaneously

You can run Remote_Control and Human_Detection_Following at the same time:

```
Terminal 1:
cd /home/navis/NAVIS/Base/Remote_Control
python app.py  # Port 5050

Terminal 2:
cd /home/navis/NAVIS/Base/Human_Detection_Following
python app.py  # Port 5051
```

**Result**:
- Both publish to `robot/control` MQTT topic
- Manual control (Remote) takes priority when active
- Robot autonomously follows when remote is idle
- Real-time command switching

---

## ğŸ“ Learning Resources

### MediaPipe Documentation
- 33 Pose Landmarks: https://mediapipe.dev/solutions/pose
- Confidence interpretation: 0.5 = 50% confidence minimum
- Visibility: 0-1 range, higher = more visible

### MQTT Protocol
- Topic: `robot/control`
- Message: JSON with `cmd` and `speed`
- QoS: 0 (fire and forget)

### OpenCV
- Frame capture: `cap.read()`
- Encoding: `cv2.imencode('.jpg', frame)`
- Streaming: MJPEG format

---

## ğŸ‰ Success Indicators

You'll know it's working when:

âœ… Web interface loads at `http://192.168.0.199:5051`
âœ… Live video appears with skeleton overlay
âœ… Status shows "PERSON DETECTED" when you're visible
âœ… Position changes as you move left/right
âœ… Depth changes as you move closer/farther
âœ… MQTT messages appear in `mosquitto_sub` output
âœ… Robot responds to commands immediately
âœ… Robot follows you around the room
âœ… FPS counter shows 15+ frames per second

---

## ğŸš€ Next Steps

1. **Deploy and Test** - Run the system with actual robot
2. **Fine-tune Thresholds** - Adjust sensitivity for your environment
3. **Optimize Performance** - Adjust resolution/settings if needed
4. **Add Features** - Gesture control, recording, cloud integration
5. **Document Results** - Record performance metrics

---

## ğŸ“ Quick Reference

**Start App**: `python app.py` (in Base/Human_Detection_Following)
**Web URL**: `http://192.168.0.199:5051`
**Test System**: `python test_setup.py`
**Monitor MQTT**: `mosquitto_sub -h 127.0.0.1 -p 1883 -t "robot/control"`
**View Skeleton**: `python viewer.py` (no web server, direct camera)

---

## ğŸ“ˆ Future Enhancements

1. Multi-person tracking (follow closest)
2. Hand gesture recognition
3. Depth calibration with known objects
4. Machine learning for behavior patterns
5. 3D pose with multiple cameras
6. Cloud data logging
7. Mobile app control
8. Computer vision obstacles detection

---

**Implementation Date**: 28 January 2026
**Status**: âœ… Complete & Ready for Deployment
**System**: Autonomous Human Following Robot
**Framework**: MediaPipe + Flask + MQTT
**Target Device**: Raspberry Pi + ESP32 + BTS7960

---

*For detailed technical information, see HUMAN_DETECTION_GUIDE.md*
*For quick setup, see QUICKSTART.md*
